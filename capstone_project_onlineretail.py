# -*- coding: utf-8 -*-
"""Capstone Project_OnlineRetail.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1XGUi1s5qAZjAZlpbaWK32QBzlyRsrO

**Data Preprocessing and Exploration**
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv("OnlineRetail.csv", encoding='ISO-8859-1')

# Display basic information about the dataset
print("Data Info:")
print(df.info())

# Show first few rows
print("\nFirst 5 rows of the dataset:")
print(df.head())

# Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Drop rows with missing CustomerID as they are essential for customer purchase pattern analysis
df = df.dropna(subset=['CustomerID'])

# Handling missing or erroneous values in the 'Description' column by filling them with 'Unknown'
df['Description'].fillna('Unknown', inplace=True)

# Check for duplicate rows and drop them if necessary
df.drop_duplicates(inplace=True)

# Convert the 'InvoiceDate' column to datetime format
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Create new features for 'Year' and 'Month' from 'InvoiceDate'
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month

# Convert 'CustomerID' to a categorical feature
df['CustomerID'] = df['CustomerID'].astype('int64')

# Descriptive statistics
print("\nDescriptive Statistics:")
print(df.describe())

# Explore the most common products purchased
print("\nMost frequently purchased products:")
print(df['Description'].value_counts().head(10))

# Explore the most frequent customers
print("\nTop 10 customers by purchase frequency:")
print(df['CustomerID'].value_counts().head(10))

#Outlier Treatment
# Import necessary library
import matplotlib.pyplot as plt
import seaborn as sns

# Select only numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns

# Plot boxplots for each numeric column to visualize outliers
plt.figure(figsize=(15, 8))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(2, len(numeric_cols)//2 + 1, i)
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.savefig('boxplots_outliers.png')  # Save the boxplot
plt.show()

#Perform Outlier Treatment Only on Columns with Outliers
# Define a function to detect and remove outliers using IQR method
def remove_outliers_iqr(df, column):
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1  # Interquartile range

    # Define the bounds for detecting outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Remove outliers by filtering values within the bounds
    df_out = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df_out

# Apply outlier treatment to columns with outliers
# Let's assume 'Quantity' and 'UnitPrice' contain outliers based on the boxplots
outlier_columns = ['Quantity', 'UnitPrice']  # Update this list based on boxplot results

df_cleaned = df.copy()

for col in outlier_columns:
    df_cleaned = remove_outliers_iqr(df_cleaned, col)

print("\nData after removing outliers in selected columns:")
print(df_cleaned.describe())

"""**Data Visualization**"""

# Create a folder to save the plots if it doesn't exist
#import os
#if not os.path.exists('/mnt/data/plots'):
    #os.makedirs('/mnt/data/plots')

# Visualize the number of purchases per year and save the plot
plt.figure(figsize=(10,6))
sns.countplot(x='Year', data=df)
plt.title('Number of Purchases Per Year')
plt.xlabel('Year')
plt.ylabel('Number of Purchases')
plt.savefig('PurchasesByYear')  # Save plot
plt.show()

# Visualize top 10 purchased products and save the plot
top_products = df['Description'].value_counts().head(10)
plt.figure(figsize=(10,8))
top_products.plot(kind='bar')
plt.title('Top 10 Purchased Products')
plt.xlabel('Product Description')
plt.ylabel('Number of Purchases')
plt.xticks(rotation=45)
plt.savefig('top_10_products.png')  # Save plot
plt.show()

#Distribution Plot (Histogram + kernel density estimate (KDE))
# Plot distribution for Quantity and UnitPrice
plt.figure(figsize=(15, 6))

# Distribution of Quantity
plt.subplot(1, 2, 1)
sns.histplot(df['Quantity'], bins=30, kde=True)
plt.title('Distribution of Quantity')
plt.xlabel('Quantity')
plt.ylabel('Frequency')

# Distribution of UnitPrice
plt.subplot(1, 2, 2)
sns.histplot(df['UnitPrice'], bins=30, kde=True)
plt.title('Distribution of Unit Price')
plt.xlabel('Unit Price')
plt.ylabel('Frequency')

plt.tight_layout()
plt.savefig('distribution_plots.png')  # Save the distribution plots
plt.show()

#Bar Plot of Total Sales per Country
# Calculate total sales (Quantity * UnitPrice)
df['TotalSales'] = df['Quantity'] * df['UnitPrice']

# Group by Country and sum total sales
country_sales = df.groupby('Country')['TotalSales'].sum().sort_values(ascending=False)

# Plot total sales per country
plt.figure(figsize=(12, 6))
country_sales.head(10).plot(kind='bar')
plt.title('Top 10 Countries by Total Sales')
plt.xlabel('Country')
plt.ylabel('Total Sales')
plt.xticks(rotation=45)
plt.savefig('top_countries_sales.png')  # Save the bar plot
plt.show()

#Time Series Analysis of Sales
# Group by InvoiceDate and sum total sales
time_series_sales = df.groupby(df['InvoiceDate'].dt.date)['TotalSales'].sum()

# Plot time series of total sales
plt.figure(figsize=(15, 6))
time_series_sales.plot()
plt.title('Total Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Total Sales')
plt.xticks(rotation=45)
plt.savefig('time_series_sales.png')  # Save the time series plot
plt.show()

#Heatmap of Sales by Month and Country
# Create a pivot table for sales by month and country
sales_pivot = df.pivot_table(values='TotalSales', index='Country', columns='Month', aggfunc='sum', fill_value=0)

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(sales_pivot, annot=True, fmt='.0f', cmap='Blues')
plt.title('Heatmap of Sales by Month and Country')
plt.xlabel('Month')
plt.ylabel('Country')
plt.savefig('sales_heatmap.png')  # Save the heatmap
plt.show()

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=[np.number])

# Check correlation between numeric variables and save the heatmap
plt.figure(figsize=(8,6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.savefig('correlation_heatmap.png')  # Save heatmap
plt.show()

# Calculate total sales for each product
product_sales = df.groupby('Description')['TotalSales'].sum().sort_values(ascending=False)

# Top 10 products by total sales
top_10_products = product_sales.head(10)
print("Top 10 products by total sales:")
print(top_10_products)

# Calculate total sales per customer
customer_sales = df.groupby('CustomerID')['TotalSales'].sum().sort_values(ascending=False)

# Top 10 customers by total sales
top_10_customers = customer_sales.head(10)
print("Top 10 customers by total sales:")
print(top_10_customers)

# Count the number of purchases per customer
customer_frequency = df['CustomerID'].value_counts()

# Descriptive statistics of customer purchase frequency
print("Customer purchase frequency statistics:")
print(customer_frequency.describe())

# Group by Year and Month to calculate monthly sales
monthly_sales = df.groupby(['Year', 'Month'])['TotalSales'].sum()

print("Monthly sales trends:")
print(monthly_sales)

# Sales by country
sales_by_country = df.groupby('Country')['TotalSales'].sum().sort_values(ascending=False)

print("Top 5 countries by total sales:")
print(sales_by_country.head(5))

# Calculate average order value per customer
df['InvoiceTotal'] = df['Quantity'] * df['UnitPrice']
aov_by_customer = df.groupby('CustomerID')['InvoiceTotal'].mean().sort_values(ascending=False)

print("Average order value (AOV) per customer:")
print(aov_by_customer.head(10))

# Filter for returned products (negative Quantity)
returned_products = df[df['Quantity'] < 0]

# Calculate the number of returns per product
product_returns = returned_products['Description'].value_counts().head(10)

print("Top 10 returned products:")
print(product_returns)

# Percentage of total sales by top customers
top_10_percent_customers = customer_sales.head(int(len(customer_sales) * 0.1)).sum()
total_sales = df['TotalSales'].sum()

# Calculate the percentage of total sales from the top 10% of customers
percent_sales_from_top_customers = (top_10_percent_customers / total_sales) * 100

print(f"Percentage of total sales from top 10% of customers: {percent_sales_from_top_customers:.2f}%")

# Calculate the time difference between purchases for each customer
df['PreviousPurchaseDate'] = df.groupby('CustomerID')['InvoiceDate'].shift(1)
df['DaysBetweenPurchases'] = (df['InvoiceDate'] - df['PreviousPurchaseDate']).dt.days

# Median days between purchases
median_days_between_purchases = df['DaysBetweenPurchases'].median()

print(f"Median number of days between purchases: {median_days_between_purchases} days")

"""**Segment the customers based on their purchasing behavior.**"""

import pandas as pd
import datetime as dt

# Set a reference date (the last date in the dataset) for Recency calculation
current_date = df['InvoiceDate'].max()

# Calculate RFM values for each customer
rfm_df = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (current_date - x.max()).days,  # Recency
    'InvoiceNo': 'count',  # Frequency
    'TotalSales': 'sum'  # Monetary
}).reset_index()

# Rename columns for clarity
rfm_df.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']

# Handle possible negative or zero Monetary values by removing or adjusting them
rfm_df = rfm_df[rfm_df['Monetary'] > 0]

# Assign RFM scores by ranking customers on Recency, Frequency, and Monetary value
rfm_df['RecencyScore'] = pd.qcut(rfm_df['Recency'], 4, labels=[4, 3, 2, 1])  # Recency: Lower is better
rfm_df['FrequencyScore'] = pd.qcut(rfm_df['Frequency'].rank(method="first"), 4, labels=[1, 2, 3, 4])  # Frequency: Higher is better
rfm_df['MonetaryScore'] = pd.qcut(rfm_df['Monetary'], 4, labels=[1, 2, 3, 4])  # Monetary: Higher is better

# Combine Recency, Frequency, and Monetary scores into a single RFM score
rfm_df['RFM_Score'] = rfm_df['RecencyScore'].astype(str) + rfm_df['FrequencyScore'].astype(str) + rfm_df['MonetaryScore'].astype(str)

# Segment customers based on RFM score
def rfm_segment(x):
    if x['RFM_Score'] == '444':
        return 'Best Customers'
    elif x['RFM_Score'] in ['344', '434', '443']:
        return 'Loyal Customers'
    elif x['RFM_Score'].startswith('1'):
        return 'At Risk'
    elif x['RFM_Score'].startswith('4'):
        return 'New Customers'
    elif x['RFM_Score'] in ['233', '322', '332']:
        return 'Potential Loyalists'
    else:
        return 'Others'

# Apply segmentation
rfm_df['Segment'] = rfm_df.apply(rfm_segment, axis=1)

# Display RFM segments with their size
rfm_summary = rfm_df.groupby('Segment').size().sort_values(ascending=False)
print("\nRFM Segmentation Summary:")
print(rfm_summary)

# Optionally, save the RFM segmentation results
rfm_df.to_csv('rfm_segments.csv', index=False)

"""**Model and it's evaluation**

Step 1: Data Preprocessing
"""

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Select RFM columns
rfm_features = rfm_df[['Recency', 'Frequency', 'Monetary']]

# Handle missing values (if any) using imputation
imputer = SimpleImputer(strategy='mean')
rfm_imputed = imputer.fit_transform(rfm_features)

# Scale the features to normalize the data
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_imputed)

print("Scaled RFM features:")
print(rfm_scaled[:5])

"""Step 2: K-Means Clustering"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Use the Elbow method to find the optimal number of clusters
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    sse.append(kmeans.inertia_)

# Plot the SSE for each value of k
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), sse, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Sum of Squared Errors (SSE)')
plt.savefig('elbow_method.png')  # Save the Elbow plot
plt.show()

"""Apply K-Means Clustering"""

# Apply KMeans with the selected number of clusters (let's assume k=4 based on the elbow method)
kmeans = KMeans(n_clusters=4, random_state=42)
rfm_df['Cluster'] = kmeans.fit_predict(rfm_scaled)

# Visualize the cluster centroids
cluster_centroids = kmeans.cluster_centers_
print("Cluster centroids (in scaled units):")
print(cluster_centroids)

"""Step 4: Model Evaluation"""

from sklearn.metrics import silhouette_score

# Calculate silhouette score for K-Means clustering
silhouette_avg = silhouette_score(rfm_scaled, rfm_df['Cluster'])
print(f'Silhouette Score for K-Means: {silhouette_avg:.3f}')

"""Step 5: Visualize Clusters"""

from sklearn.decomposition import PCA

# Perform PCA to reduce dimensions to 2D for visualization
pca = PCA(n_components=2)
rfm_pca = pca.fit_transform(rfm_scaled)

# Add PCA results to the RFM dataframe
rfm_df['PCA1'] = rfm_pca[:, 0]
rfm_df['PCA2'] = rfm_pca[:, 1]

# Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=rfm_df, palette='Set2', s=100)
plt.title('Customer Segments Visualized Using PCA')
plt.savefig('customer_segments_pca.png')  # Save the cluster plot
plt.show()

"""Step 6: Interpret the Segments"""

# Calculate the mean RFM values for each cluster
cluster_analysis = rfm_df.groupby('Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean'
}).reset_index()

print("Cluster Analysis (mean RFM values):")
print(cluster_analysis)
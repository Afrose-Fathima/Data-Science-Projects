# -*- coding: utf-8 -*-
"""Capstone Project_Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FllQvDGOwt7d4cCC6wJJysxV1SPOF70
"""

import pandas as pd

# Load the dataset
df = pd.read_csv("Reviews (1).csv")

# Preview the dataset
print(df.head())

"""**Data Preprocessing**"""

# Check for missing values
missing_values = df.isnull().sum()

# Print missing values by column
print(missing_values)

# Handling missing values: Removing rows with missing reviews
df = df.dropna(subset=['ProfileName', 'Summary'])  # Replace 'review_text' with the actual review column name

# Optionally, fill missing values in other columns
# Example: df['rating'].fillna(df['rating'].median(), inplace=True)

"""**Text Cleaning (Preprocessing Review Text)**"""

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Ensure that NLTK stopwords are downloaded
import nltk
nltk.download('stopwords')
nltk.download('punkt')

# Function to clean text
def clean_text(text):
    # Remove punctuation and special characters
    text = re.sub(r'[^\w\s]', '', text)

    # Tokenize words
    words = word_tokenize(text.lower())  # Convert to lowercase and tokenize

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Join words back into a cleaned sentence
    return ' '.join(words)

# Apply the cleaning function to the review column
df['cleaned_review'] = df['Text'].apply(clean_text)

"""**3. Handling Outliers**"""

# Detecting outliers in all numeric columns using the IQR method
def detect_outliers(df):
    outlier_columns = []  # List to store columns with outliers
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns  # Selecting numeric columns

    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Check for outliers in the column
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

        if not outliers.empty:
            outlier_columns.append(col)
            print(f"Outliers detected in column: {col}")
            print(f"Number of outliers in {col}: {outliers.shape[0]}")

    return outlier_columns

# Call the function to detect outliers in the dataset
outlier_columns = detect_outliers(df)

# Print the columns with outliers
print("Columns with outliers:", outlier_columns)

# Optionally: Remove outliers for all columns with outliers
for col in outlier_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter out the outliers
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

# After removing outliers, check the shape of the dataframe
print(f"Data shape after outlier removal: {df.shape}")

"""**4. Exploratory Data Analysis (EDA)**

Descriptive Statistics
"""

# Summary statistics for numerical columns (e.g., rating)
print(df.describe())

"""Distribution of Ratings"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the distribution of ratings
sns.histplot(df['Score'], bins=10, kde=True)
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

"""Word Frequency Analysis"""

from collections import Counter

# Create a list of all words from the cleaned reviews
all_words = ' '.join(df['cleaned_review']).split()

# Get the most common words
word_freq = Counter(all_words).most_common(20)

# Convert to DataFrame for easier plotting
word_freq_df = pd.DataFrame(word_freq, columns=['Word', 'Frequency'])

# Plot word frequency
sns.barplot(x='Frequency', y='Word', data=word_freq_df)
plt.title('Top 20 Most Common Words')
plt.savefig('word_frequency.png')
plt.show()

"""Sentiment Analysis"""

from textblob import TextBlob

# Function to get sentiment polarity
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

# Apply sentiment analysis
df['sentiment'] = df['cleaned_review'].apply(get_sentiment)

# Visualize sentiment distribution
sns.histplot(df['sentiment'], bins=20, kde=True)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment Polarity')
plt.ylabel('Frequency')
plt.savefig('sentiment_distribution.png')
plt.show()

"""**Data Visualization**"""

import seaborn as sns
import matplotlib.pyplot as plt

# Get the top N products based on the number of reviews
top_n = 20  # Change this number as needed
top_products = df['ProductId'].value_counts().head(top_n).index

# Filter the DataFrame for the top N products
filtered_df = df[df['ProductId'].isin(top_products)]

plt.figure(figsize=(12, 6))
sns.boxplot(x='ProductId', y='Score', data=filtered_df)

plt.title('Rating Distribution by Product (Top N Products)')
plt.xlabel('Product ID')
plt.ylabel('Rating')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('rating_distribution_by_top_products.png')  # Save the figure
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Get the top N products based on the number of reviews or average score
top_n = 20  # Change this to the number of products you want to display
top_products = df['ProductId'].value_counts().head(top_n).index

# Filter the DataFrame for the top N products
filtered_df = df[df['ProductId'].isin(top_products)]

plt.figure(figsize=(12, 6))
sns.boxplot(x='ProductId', y='Score', data=filtered_df)

plt.title('Sentiment Polarity by Product (Top N Products)')
plt.xlabel('Product ID')
plt.ylabel('Sentiment Polarity')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('sentiment_by_top_products.png')  # Save the figure
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
# Get the number of reviews per product and sort it
top_n = 20  # Change this to the number of products you want to display
top_reviews_per_product = df['ProductId'].value_counts().head(top_n)

plt.figure(figsize=(10, 6))
top_reviews_per_product.plot(kind='bar')

plt.title(f'Top {top_n} Products by Number of Reviews')
plt.xlabel('Product ID')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('top_reviews_per_product.png')  # Save the figure
plt.show()

# 4. Relationship Between Rating and Sentiment
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Score', y='sentiment', data=df)
plt.title('Rating vs Sentiment')
plt.xlabel('Rating')
plt.ylabel('Sentiment Polarity')
plt.savefig('rating_vs_sentiment.png')  # Save the figure
plt.show()

# 4. Relationship Between Rating and Sentiment with Regression Line
plt.figure(figsize=(10, 6))
sns.regplot(x='Score', y='sentiment', data=df, scatter_kws={'alpha': 0.3, 's': 10}, line_kws={'color': 'red'})

plt.title('Rating vs Sentiment with Regression Line')
plt.xlabel('Rating')
plt.ylabel('Sentiment Polarity')
plt.tight_layout()
plt.savefig('rating_vs_sentiment_with_regression.png')  # Save the figure
plt.show()

# 5. Word Cloud of Frequent Words in Reviews
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Create a single string of all words in 'cleaned_review' column
all_words = ' '.join(df['cleaned_review'])

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis labels
plt.title('Word Cloud of Frequent Words in Reviews')
plt.savefig('word_cloud.png')  # Save the figure
plt.show()

"""**1. Find various trends and patterns in the reviews data, create useful insights that best describe the product quality.**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Convert rating to a binary classification:
# 1 = High Quality (rating >= 4), 0 = Low Quality (rating < 4)
df['product_quality'] = np.where(df['Score'] >= 4, 1, 0)

# 2. Text Vectorization using TF-IDF
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X = tfidf.fit_transform(df['cleaned_review']).toarray()

# Features: Review text and potentially other features like 'product_id'
X = pd.DataFrame(X)

# Target: Product quality (1 = high, 0 = low)
y = df['product_quality']

# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Model Selection - Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 5. Model Evaluation
y_pred = rf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 6. Feature Importance - Top Words Impacting Predictions
feature_importances = rf.feature_importances_
top_n = 20  # Show top 20 features (words)

# Get the top 20 important words
indices = np.argsort(feature_importances)[-top_n:]
features = np.array(tfidf.get_feature_names_out())[indices]

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances[indices], y=features)
plt.title("Top Words Impacting Product Quality Predictions")
plt.savefig('top_words_product_quality.png')  # Save the figure
plt.show()

# 7. Insights from Model:
# - Words with the highest impact on classification will show up in the above plot.

"""**Trends and Patterns:**

Rating and Sentiment Correlation:
"""

# Analyzing correlation between ratings and sentiment polarity
correlation = df['Score'].corr(df['sentiment'])
print(f"Correlation between Rating and Sentiment: {correlation}")

"""Review Length vs. Product Quality:
Longer reviews may indicate more detailed feedback, often associated with extreme ratings (very good or very bad). We can analyze if longer reviews tend to skew towards one quality class more than the other.
"""

# Calculate review length (word count)
df['review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))

# Visualize review length vs. product quality
plt.figure(figsize=(10, 6))
sns.boxplot(x='product_quality', y='review_length', data=df)
plt.title('Review Length vs. Product Quality')
plt.savefig('review_length_vs_quality.png')  # Save the figure
plt.show()

"""Product Review Patterns:"""

# Get top 20 products based on average rating
top_n = 20
avg_rating_per_product = df.groupby('ProductId')['Score'].mean().sort_values(ascending=False).head(top_n)

plt.figure(figsize=(10, 6))
avg_rating_per_product.plot(kind='bar')
plt.title('Top 20 Average Ratings per Product')
plt.xlabel('Product ID')
plt.ylabel('Average Rating')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('top_avg_rating_per_product.png')
plt.show()

"""**2. Classify each review based on the sentiment associated with the same.**"""

from textblob import TextBlob

# Assuming 'df' contains a 'cleaned_review' column with preprocessed text

# 1. Function to get sentiment polarity
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

# 2. Apply sentiment analysis to each review
df['sentiment_polarity'] = df['cleaned_review'].apply(get_sentiment)

# 3. Classify sentiment based on the polarity score
def classify_sentiment(polarity):
    if polarity > 0:
        return 'Positive'
    elif polarity < 0:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment_class'] = df['sentiment_polarity'].apply(classify_sentiment)

# Print the first few rows to check sentiment classification
print(df[['cleaned_review', 'sentiment_polarity', 'sentiment_class']].head())

# 4. Visualize the distribution of sentiments
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.countplot(x='sentiment_class', data=df)
plt.title('Sentiment Classification Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.savefig('sentiment_class_distribution.png')  # Save the figure
plt.show()

# Optionally, you can export the results with sentiment classifications
df[['cleaned_review', 'sentiment_polarity', 'sentiment_class']].to_csv('reviews_with_sentiment.csv', index=False)